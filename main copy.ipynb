{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "os.environ.get('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating funtion to load a just a pdf document \n",
    "\n",
    "# def load_document(file):\n",
    "#     from langchain.document_loaders import PyPDFLoader\n",
    "#     print(f'loading...{file}')\n",
    "#     loader=PyPDFLoader(file)\n",
    "#     data=loader.load_and_split()\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d= load_document(r'C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.pdf')\n",
    "# #d\n",
    "# #d[0].page_content\n",
    "# # d[0].metadata\n",
    "# # for v in d:\n",
    "# #     print(v)  ( # manly for lazy load as it is good for any but manily for lazy load ( you can use v.metadata for metadata) )\n",
    "# print(f'you have {len(d)} pages in your data')\n",
    "# print(f'there are {len(d[0].page_content)} characters in the page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function load different range of funtions \n",
    "# need to install pypdf and Doc2tetloder\n",
    "\n",
    "def load_any_document(file):\n",
    "    # this will get the extention from the file name ex:\".pdf\" \".docx\" and many more \n",
    "    import os \n",
    "    name,extention= os.path.splitext(file)\n",
    "    # this loads pdf\n",
    "    if extention==\".pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'loading...{file}')\n",
    "        loader=PyPDFLoader(file)\n",
    "        #this loads docx file\n",
    "    elif extention=='.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f\"loadings {file=}\")\n",
    "        loader=Docx2txtLoader(file)\n",
    "        # you can add many other formats as you needed using elif funtion\n",
    "        # this below code will return none and intimate that the given document was not supported \n",
    "    else:\n",
    "        print(\"Document format is not Supporting\")\n",
    "        return None\n",
    "    # this is loading the data fron the file and returning data and compliting the function \n",
    "    data=loader.load_and_split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets try loading docxs now \n",
    "# data=load_any_document(r'C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.docx')\n",
    "# #data\n",
    "# data[0].page_content\n",
    "# data[0].metadata\n",
    "# print(f'This document has {len(data)} pages')\n",
    "# print(f'each page has {len(data[0].page_content)} characters per page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now lets connect to online sources \n",
    "# # needed to install wekipedia \n",
    "\n",
    "\n",
    "# def load_from_wikipedia(query,lang='en',load_max_docs=2):\n",
    "#     from langchain.document_loaders import WikipediaLoader\n",
    "#     loader=WikipediaLoader(query=query,lang=lang,load_max_docs=load_max_docs)\n",
    "#     data=loader.load()\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=load_from_wikipedia('GPT4')\n",
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking is very importent it is the only thing that can make you get revelent answer from the llm \n",
    "# when you chunk or split the page # Rule of Tumb is it has to be readable by a human \n",
    "def chunk_data(data,chunk_size=100):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap  = 0,\n",
    "        length_function = len,\n",
    "        )\n",
    "    chunks=text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d= load_document(r'C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.pdf')\n",
    "# print(f'you have {len(d)} pages in your data')\n",
    "# print(f'there are {len(d[0].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks=chunk_data(d,chunk_size=2000)\n",
    "# print(len(chunks))\n",
    "# print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and uploading to pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings (index_name,chunks=None):\n",
    "    import os\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "    embedding=OpenAIEmbeddings()\n",
    "\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exits , loading embeddings')\n",
    "        vectorstore = Pinecone.from_existing_index(index_name,embedding)\n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'creating new index {index_name}')\n",
    "        pinecone.create_index(index_name,dimension=1536,metric='cosine')\n",
    "        vectorstore=Pinecone.from_documents(chunks,embedding,index_name=index_name)\n",
    "        print('ok')\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to delete any indexes \n",
    "\n",
    "def delete_pinecone_index(index_name='all'):\n",
    "    import os\n",
    "    import pinecone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name=='all':\n",
    "        indexs=pinecone.list_indexes()\n",
    "        print(f' Deleting all indexes')\n",
    "        for index in indexs:\n",
    "            pinecone.delete_index(index)\n",
    "            \n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'deleting {index_name}')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('ok')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...File/aptitude.pdf\n",
      "This document has 58 pages\n",
      "each page has 643 characters per page\n"
     ]
    }
   ],
   "source": [
    "# lets try loading docxs now \n",
    "data=load_any_document(r'File/aptitude.pdf')\n",
    "#data\n",
    "data[0].page_content\n",
    "data[0].metadata\n",
    "print(f'This document has {len(data)} pages')\n",
    "print(f'each page has {len(data[0].page_content)} characters per page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "General awareness and Aptitude test    3\n",
      "12.27, 24, 30, 27, 33,______\n",
      "a.48 b.27\n",
      "c.30 d.24\n",
      "13.4, 10, 22, 46,_______\n",
      "a.56 b.66\n",
      "c.76 d.94\n",
      "14.4, 1, -8,_______,-20, 29, -32, 43\n",
      "a.15 b.16\n",
      "c.17 d.18\n",
      "15.2, -2, 7, 3, 17,_______,32, 13\n",
      "a.70 b.10\n",
      "c.9 d.8\n",
      "16.9, 15, 23, 33,_______\n",
      "a.44 b.36\n",
      "c.38 d.45\n"
     ]
    }
   ],
   "source": [
    "chunks=chunk_data(data,chunk_size=300)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking and getting answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_get_answers(vector_store,q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0.7,verbose=True)\n",
    "    retriever=vector_store.as_retriever(search_type='similarity' , search_kwargs={'k':3})\n",
    "    # retriver=vector_store.similarity_search(query=q,k=3)\n",
    "    chain=RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=retriever)\n",
    "    answer=chain.run(q)\n",
    "    return answer\n",
    "\n",
    "# ask and get answers + memory \n",
    "\n",
    "def ask_with_memory(vector_store,question,chat_history=[]):\n",
    "    # we are importing Conversational retrivel because \n",
    "    # it is build on RetervalQ&A\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0.7,verbose=True)\n",
    "    retriver=vector_store.as_retriever(search_type='similarity' , search_kwargs={'k':3})\n",
    "    crc=ConversationalRetrievalChain.from_llm(llm,retriever=retriver)\n",
    "    result=crc({'question':question,'chat_history':chat_history})\n",
    "    chat_history.append((question,result['answer']))\n",
    "    return result,chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\.virtualenvs\\PrivateGpt-MZja8mkQ\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deleting all indexes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m delete_pinecone_index()\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mdelete_pinecone_index\u001b[1;34m(index_name)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m Deleting all indexes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m indexs:\n\u001b[1;32m---> 12\u001b[0m         pinecone\u001b[39m.\u001b[39;49mdelete_index(index)\n\u001b[0;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mok\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\deepa\\.virtualenvs\\PrivateGpt-MZja8mkQ\\lib\\site-packages\\pinecone\\manage.py:171\u001b[0m, in \u001b[0;36mdelete_index\u001b[1;34m(name, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m get_remaining():\n\u001b[1;32m--> 171\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mwhile\u001b[39;00m get_remaining() \u001b[39mand\u001b[39;00m timeout \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index askadocument already exits , loading embeddings\n",
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='government departments\\na)f b)g\\nc)h d)e', metadata={'page': 19.0, 'source': 'File/aptitude.pdf'}),\n",
       " Document(page_content='1. c 2. b 3.a\\n4. b 5. a 6.b\\n7. c 8. b 9.a\\n10. a 11. a 12.b\\n13. c 14. a 15.c\\n16. b 17. a 18.c\\n19. c 20. b', metadata={'page': 32.0, 'source': 'File/aptitude.pdf'}),\n",
       " Document(page_content='General awareness and Aptitude test    25\\nVOCABULAR Y TEST\\nImportant Points\\nIn this test the candidates are required to\\nform new words by suffixing or prefixing letters,\\nthe section also tests the basic knowledge of\\nEnglish language\\nIn question 1 to 3 find the odd one out.\\n1)a)Tiger b)Leopard', metadata={'page': 24.0, 'source': 'File/aptitude.pdf'})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name='askadocument'\n",
    "vectorstore = insert_or_fetch_embeddings(index_name,chunks=chunks)\n",
    "q='what is the whole document about'\n",
    "\n",
    "vectorstore.similarity_search(query=q,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document appears to be a test or assessment, specifically a \"General awareness and Aptitude test\" with a section on \"Vocabulary Test\". The document provides some important points about the test, including the requirement to form new words by suffixing or prefixing letters and the testing of basic knowledge of the English language. It also mentions that the test includes questions where the candidate needs to find the odd one out.\n"
     ]
    }
   ],
   "source": [
    "q='what is the whole document about'\n",
    "answer=ask_get_answers(vector_store=vectorstore,q=q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a loop so user can ask questions continuesly \n",
    "\n",
    "import time\n",
    "i=1\n",
    "print('write quit or exit to Exit ')\n",
    "while True:\n",
    "    q=input(f'Question# {i}')\n",
    "    i+=1\n",
    "    if q.lower() in ['quit','exit']:\n",
    "        print('Qutting .. the session')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer=ask_get_answers(vector_store,q)\n",
    "    print(f'\\ngAnswer:{answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index askadocument already exits , loading embeddings\n",
      "ok\n",
      "Based on the given context, the different topics in the book could include general awareness and aptitude test questions related to various subjects such as war, currency, binding, countries, analogies, and problem figures.\n",
      "[('what are th different topics in the Book', 'Based on the given context, the different topics in the book could include general awareness and aptitude test questions related to various subjects such as war, currency, binding, countries, analogies, and problem figures.')]\n"
     ]
    }
   ],
   "source": [
    "# asking with memory \n",
    "chat_history=[]\n",
    "question='what are th different topics in the Book'\n",
    "vectorstore=insert_or_fetch_embeddings('askadocument')\n",
    "result,chat_history=ask_with_memory(vector_store=vectorstore,question=question,chat_history=chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index askadocument already exits , loading embeddings\n",
      "ok\n",
      "1) Which countries are part of the G7 economic group?\n",
      "a) United States\n",
      "b) Japan\n",
      "c) Germany\n",
      "d) France\n",
      "e) Italy\n",
      "f) United Kingdom\n",
      "g) Canada\n",
      "\n",
      "2) Which countries are members of the BRICS association?\n",
      "a) Brazil\n",
      "b) Russia\n",
      "c) India\n",
      "d) China\n",
      "e) South Africa\n",
      "[('give me 2 multiple answer questions on countries', '1) Which countries are part of the G7 economic group?\\na) United States\\nb) Japan\\nc) Germany\\nd) France\\ne) Italy\\nf) United Kingdom\\ng) Canada\\n\\n2) Which countries are members of the BRICS association?\\na) Brazil\\nb) Russia\\nc) India\\nd) China\\ne) South Africa')]\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "question='give me 2 multiple answer questions on countries'\n",
    "vectorstore=insert_or_fetch_embeddings('askadocument')\n",
    "result,chat_history=ask_with_memory(vector_store=vectorstore,question=question,chat_history=chat_history)\n",
    "print(result['answer'])\n",
    "print(chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
