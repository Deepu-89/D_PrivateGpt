{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "os.environ.get('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\deepa\\.virtualenvs\\privategpt-mzja8mkq\\lib\\site-packages (3.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating funtion to load a just a pdf document \n",
    "\n",
    "def load_document(file):\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    print(f'loading...{file}')\n",
    "    loader=PyPDFLoader(file)\n",
    "    data=loader.load_and_split()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.pdf\n",
      "you have 58 pages in your data\n",
      "there are 643 characters in the page\n"
     ]
    }
   ],
   "source": [
    "d= load_document(r'C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.pdf')\n",
    "#d\n",
    "#d[0].page_content\n",
    "# d[0].metadata\n",
    "# for v in d:\n",
    "#     print(v)  ( # manly for lazy load as it is good for any but manily for lazy load ( you can use v.metadata for metadata) )\n",
    "print(f'you have {len(d)} pages in your data')\n",
    "print(f'there are {len(d[0].page_content)} characters in the page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\deepa\\.virtualenvs\\privategpt-mzja8mkq\\lib\\site-packages (3.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in c:\\users\\deepa\\.virtualenvs\\privategpt-mzja8mkq\\lib\\site-packages (0.8)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function load different range of funtions \n",
    "\n",
    "def load_any_document(file):\n",
    "    # this will get the extention from the file name ex:\".pdf\" \".docx\" and many more \n",
    "    import os \n",
    "    name,extention= os.path.splitext(file)\n",
    "    # this loads pdf\n",
    "    if extention==\".pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'loading...{file}')\n",
    "        loader=PyPDFLoader(file)\n",
    "        #this loads docx file\n",
    "    elif extention=='.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f\"loadings {file=}\")\n",
    "        loader=Docx2txtLoader(file)\n",
    "        # you can add many other formats as you needed using elif funtion\n",
    "        # this below code will return none and intimate that the given document was not supported \n",
    "    else:\n",
    "        print(\"Document format is not Supporting\")\n",
    "        return None\n",
    "    # this is loading the data fron the file and returning data and compliting the function \n",
    "    data=loader.load_and_split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadings file='C:\\\\Users\\\\deepa\\\\Documents\\\\langchain_new\\\\PrivateGpt\\\\File\\\\aptitude.docx'\n",
      "This document has 22 pages\n",
      "each page has 3985 characters per page\n"
     ]
    }
   ],
   "source": [
    "# # lets try loading docxs now \n",
    "# data=load_any_document(r'C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.docx')\n",
    "# #data\n",
    "# data[0].page_content\n",
    "# data[0].metadata\n",
    "# print(f'This document has {len(data)} pages')\n",
    "# print(f'each page has {len(data[0].page_content)} characters per page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now lets connect to online sources \n",
    "# # needed to install wekipedia \n",
    "\n",
    "\n",
    "# def load_from_wikipedia(query,lang='en',load_max_docs=2):\n",
    "#     from langchain.document_loaders import WikipediaLoader\n",
    "#     loader=WikipediaLoader(query=query,lang=lang,load_max_docs=load_max_docs)\n",
    "#     data=loader.load()\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its numbered \"GPT-n\" series of GPT foundation models. It was released on March 14, 2023, and has been made publicly available in a limited form via the chatbot product ChatGPT Plus (a premium version of ChatGPT), and with access to the GPT-4 based version of OpenAI's API being provided via a waitlist. As a transformer based model, GPT-4 was pretrained to predict the next token (using both public data and \"data licensed from third-party providers\"), and was then fine-tuned with reinforcement learning from human and AI feedback for human alignment and policy compliance.: 2 Observers reported the GPT-4 based version of ChatGPT to be an improvement on the previous (GPT-3.5 based) ChatGPT, with the caveat that GPT-4 retains some of the same problems. Unlike the predecessors, GPT-4 can take images as well as text as input. OpenAI has declined to reveal technical information such as the size of the GPT-4 model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT. \n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hour or so\". On a test of 89 sec\n"
     ]
    }
   ],
   "source": [
    "# data=load_from_wikipedia('GPT4')\n",
    "# print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking is very importent it is the only thing that can make you get revelent answer from the llm \n",
    "# when you chunk or split the page # Rule of Tumb is it has to be readable by a human \n",
    "def chunk_data(data,chunk_size=100):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap  = 0,\n",
    "        length_function = len,\n",
    "        )\n",
    "    chunks=text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d= load_document(r'C:\\Users\\deepa\\Documents\\langchain_new\\PrivateGpt\\File\\aptitude.pdf')\n",
    "# print(f'you have {len(d)} pages in your data')\n",
    "# print(f'there are {len(d[0].page_content)} characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks=chunk_data(d,chunk_size=2000)\n",
    "# print(len(chunks))\n",
    "# print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and uploading to pinecone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings (index_name,chunks=None):\n",
    "    import os\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "    embedding=OpenAIEmbeddings()\n",
    "\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "\n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exits , loading embeddings')\n",
    "        vectorstore = Pinecone.from_existing_index(index_name,embedding)\n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'creating new index {index_name}')\n",
    "        pinecone.create_index(index_name,dimension=1536,metric='cosine')\n",
    "        vectorstore=Pinecone.from_documents(chunks,embedding,index_name=index_name)\n",
    "        print('ok')\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to delete any indexes \n",
    "\n",
    "def delete_pinecone_index(index_name='all'):\n",
    "    import os\n",
    "    import pinecone\n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "\n",
    "    if index_name=='all':\n",
    "        indexs=pinecone.list_indexes()\n",
    "        print(f' Deleting all indexes')\n",
    "        for index in indexs:\n",
    "            pinecone.delete_index(index)\n",
    "            \n",
    "        print('ok')\n",
    "    else:\n",
    "        print(f'deleting {index_name}')\n",
    "        pinecone.delete_index(index_name)\n",
    "        print('ok')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...File/aptitude.pdf\n",
      "This document has 58 pages\n",
      "each page has 643 characters per page\n"
     ]
    }
   ],
   "source": [
    "# lets try loading docxs now \n",
    "data=load_any_document(r'File/aptitude.pdf')\n",
    "#data\n",
    "data[0].page_content\n",
    "data[0].metadata\n",
    "print(f'This document has {len(data)} pages')\n",
    "print(f'each page has {len(data[0].page_content)} characters per page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "General awareness and Aptitude test    3\n",
      "12.27, 24, 30, 27, 33,______\n",
      "a.48 b.27\n",
      "c.30 d.24\n",
      "13.4, 10, 22, 46,_______\n",
      "a.56 b.66\n",
      "c.76 d.94\n",
      "14.4, 1, -8,_______,-20, 29, -32, 43\n",
      "a.15 b.16\n",
      "c.17 d.18\n",
      "15.2, -2, 7, 3, 17,_______,32, 13\n",
      "a.70 b.10\n",
      "c.9 d.8\n",
      "16.9, 15, 23, 33,_______\n",
      "a.44 b.36\n",
      "c.38 d.45\n"
     ]
    }
   ],
   "source": [
    "chunks=chunk_data(data,chunk_size=300)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deleting all indexes\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating new index askadocument\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "index_name='askadocument'\n",
    "vectorstore = insert_or_fetch_embeddings(index_name,chunks=chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking and getting answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_get_answers(vector_store,q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0.7,verbose=True)\n",
    "    retriver=vector_store.as_retriver(search_type='similarity' , search_kwargs={'k':3})\n",
    "    chain=RetrievalQA.from_chain_type(llm,chain_type='stuff',retriver=retriver)\n",
    "    answer=chain.run(q)\n",
    "    return answer\n",
    "\n",
    "# ask and get answers + memory \n",
    "\n",
    "def ask_with_memory(vector_store,question,chat_history=[]):\n",
    "    # we are importing Conversational retrivel because \n",
    "    # it is build on RetervalQ&A\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm=ChatOpenAI(model='gpt-3.5-turbo',temperature=0.7,verbose=True)\n",
    "    retriver=vector_store.as_retriver(search_type='similarity' , search_kwargs={'k':3})\n",
    "    crc=ConversationalRetrievalChain.from_llm(llm,retriever=retriver)\n",
    "    result=crc({'question':question,'chat_history':chat_history})\n",
    "    chat_history.append((question,result['answer']))\n",
    "    return result,chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write quit or exit to Exit \n",
      "Qutting .. the session\n"
     ]
    }
   ],
   "source": [
    "# creating a loop so user can ask questions continuesly \n",
    "\n",
    "import time\n",
    "i=1\n",
    "print('write quit or exit to Exit ')\n",
    "while True:\n",
    "    q=input(f'Question# {i}')\n",
    "    i+=1\n",
    "    if q.lower() in ['quit','exit']:\n",
    "        print('Qutting .. the session')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer=ask_get_answers(vector_store,q)\n",
    "    print(f'\\ngAnswer:{answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asking with memory \n",
    "chat_history=[]\n",
    "question='what are th different topics in the Book'\n",
    "result,chat_history=ask_with_memory(vector_store,question=question,chat_history=chat_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
